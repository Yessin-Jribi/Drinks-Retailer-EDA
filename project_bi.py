# -*- coding: utf-8 -*-
"""PROJECT BI

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1t-cC7lk-xAzlgbLkfzCMOMEIbAarVqvu
"""

import pandas as pd
from google.colab import files
uploaded = files.upload()


# Load inventory data
ending_inventory = pd.read_csv('EndInvFINAL12312016.csv')

ending_inventory.head()

beginning_inventory = pd.read_csv('BegInvFINAL12312016.csv')

beginning_inventory.head()

purchases_data  = pd.read_csv('PurchasesFINAL12312016.csv')

purchases_data.head()

import pandas as pd
from google.colab import files
uploaded = files.upload()


# Load inventory data
df= pd.read_csv('BegInvFINAL12312016.csv')

import pandas as pd
import numpy as np
import re



# Replace Missing Values with Column Averages (Numeric Columns)
numeric_cols = ["onHand", "Price"]  # Columns where we can calculate the average
for col in numeric_cols:
    df[col].fillna(df[col].mean(), inplace=True)  # Fill missing values with the column average

# Remove Duplicate Rows
df.drop_duplicates(inplace=True)

# Standardize Date Format
# Assuming there's a 'Date' column (replace with actual date column name)
if 'startDate' in df.columns:
    df['startDate'] = pd.to_datetime(df['startDate'], errors='coerce')  # Convert to datetime format
    df['startDate'] = df['startDate'].dt.strftime('%d-%m-%Y')  # Convert format to day-month-year

# Standardize Size Format
def standardize_size(size):
    """ Convert mL to L and Liter to 1L """
    if pd.isna(size):
        return np.nan  # Keep NaN as NaN
    size = str(size).lower().strip()  # Convert to lowercase and remove spaces
    if "ml" in size:
        num = re.findall(r'\d+', size)  # Extract numbers
        if num:
            return f"{int(num[0]) / 1000}L"  # Convert mL to L
    elif "liter" in size or "litre" in size:
        return "1L"  # Standardize all "Liter" mentions to "1L"
    return size  # Return unchanged if no match

df["Size"] = df["Size"].apply(standardize_size)


#Display Sample of Cleaned Data
print(df.head())

df= pd.read_csv('BegInvFINAL12312016.csv')
print(df.head())

df= pd.read_csv('EndInvFINAL12312016.csv')
print(df.head())

import pandas as pd
import numpy as np
import re
import json
from google.colab import files

uploaded = files.upload()
salesfinal = list(uploaded.keys())[0]

# ✅ Load JSON file
with open(salesfinal, 'r') as f:
    data = json.load(f)

# ✅ Convert JSON data to a pandas DataFrame
df = pd.DataFrame(data)

# ✅ 1️⃣ Replace Missing Values with Column Averages (Numeric Columns)
numeric_cols = ["SalesQuantity", "SalesDollars", "SalesPrice", "Volume"]
for col in numeric_cols:
    if col in df.columns:
        df[col] = pd.to_numeric(df[col], errors='coerce')  # Ensure numeric type
        df[col].fillna(df[col].mean(), inplace=True)  # Fill missing values with the column average

# ✅ 2️⃣ Remove Duplicate Rows
df.drop_duplicates(inplace=True)

# ✅ 3️⃣ Standardize Date Format for 'SalesDate'
if 'SalesDate' in df.columns:
    df['SalesDate'] = pd.to_datetime(df['SalesDate'], errors='coerce')  # Convert to datetime format
    df['SalesDate'] = df['SalesDate'].dt.strftime('%d-%m-%Y')  # Convert format to day-month-year

# ✅ 4️⃣ Standardize Size Format (Convert mL to L, Liter to 1L)
def standardize_size(size):
    """ Convert mL to L and Liter to 1L """
    if pd.isna(size):
        return np.nan  # Keep NaN as NaN
    size = str(size).lower().strip()  # Convert to lowercase and remove spaces
    if "ml" in size:
        num = re.findall(r'\d+', size)  # Extract numbers
        if num:
            return f"{int(num[0]) / 1000}L"    # Convert mL to L
    elif "liter" in size or "litre" in size:
        return "1L"  # Standardize all "Liter" mentions to "1L"
    return size  # Return unchanged if no match

if 'Size' in df.columns:
    df["Size"] = df["Size"].apply(standardize_size)


# ✅ 6️⃣ Display Sample of Cleaned Data
print(df.head())

import pandas as pd
import numpy as np
import re
import json
salesfinal = list(uploaded.keys())[0]

# ✅ Load JSON file, handling potential JSON Lines format
data = []
with open(salesfinal, 'r') as f:
    for line in f:
        try:
            data.append(json.loads(line))  # Load each line as a separate JSON object
        except json.JSONDecodeError as e:
            print(f"Warning: Skipping invalid JSON line: {line.strip()} - Error: {e}")

#  Convert JSON data to a pandas DataFrame
df = pd.DataFrame(data)
# Replace Missing Values with Column Averages (Numeric Columns)
numeric_cols = ["SalesQuantity", "SalesDollars", "SalesPrice", "Volume"]
for col in numeric_cols:
    if col in df.columns:
        df[col] = pd.to_numeric(df[col], errors='coerce')  # Ensure numeric type
        df[col].fillna(df[col].mean(), inplace=True)  # Fill missing values with the column average

# Remove Duplicate Rows
df.drop_duplicates(inplace=True)

#  Standardize Date Format for 'SalesDate'
if 'SalesDate' in df.columns:
    df['SalesDate'] = pd.to_datetime(df['SalesDate'], errors='coerce')  # Convert to datetime format
    df['SalesDate'] = df['SalesDate'].dt.strftime('%d-%m-%Y')  # Convert format to day-month-year

#  Standardize Size Format (Convert mL to L, Liter to 1L)
def standardize_size(size):
    """ Convert mL to L and Liter to 1L """
    if pd.isna(size):
        return np.nan  # Keep NaN as NaN
    size = str(size).lower().strip()  # Convert to lowercase and remove spaces
    if "ml" in size:
        num = re.findall(r'\d+', size)  # Extract numbers
        if num:
            return f"{int(num[0]) / 1000}L"    # Convert mL to L
    elif "liter" in size or "litre" in size:
        return "1L"  # Standardize all "Liter" mentions to "1L"
    return size  # Return unchanged if no match

if 'Size' in df.columns:
    df["Size"] = df["Size"].apply(standardize_size)


# Display Sample of Cleaned Data
print(df.head())

from google.colab import files

files.download(salesfinal)

import pandas as pd
from google.colab import files
uploaded = files.upload()


# Load inventory data
df= pd.read_csv('EndInvFINAL12312016.csv')

import pandas as pd
import numpy as np
import re



# Replace Missing Values with Column Averages (Numeric Columns)
numeric_cols = ["onHand", "Price"]  # Columns where we can calculate the average
for col in numeric_cols:
    df[col].fillna(df[col].mean(), inplace=True)  # Fill missing values with the column average

# Remove Duplicate Rows
df.drop_duplicates(inplace=True)

# Standardize Date Format
# Assuming there's a 'Date' column (replace with actual date column name)
if 'endDate' in df.columns:
    df['endDate'] = pd.to_datetime(df['endDate'], errors='coerce')  # Convert to datetime format
    df['endDate'] = df['endDate'].dt.strftime('%d-%m-%Y')  # Convert format to day-month-year

# Standardize Size Format
def standardize_size(size):
    """ Convert mL to L and Liter to 1L """
    if pd.isna(size):
        return np.nan  # Keep NaN as NaN
    size = str(size).lower().strip()  # Convert to lowercase and remove spaces
    if "ml" in size:
        num = re.findall(r'\d+', size)  # Extract numbers
        if num:
            return f"{int(num[0]) / 1000}L"  # Convert mL to L
    elif "liter" in size or "litre" in size:
        return "1L"  # Standardize all "Liter" mentions to "1L"
    return size  # Return unchanged if no match

df["Size"] = df["Size"].apply(standardize_size)


#Display Sample of Cleaned Data
print(df.head())

from google.colab import files
# Upload the ZIP file
uploaded = files.upload()  # This will open the file upload dialog

import zipfile
import os

# Extract the uploaded ZIP file
zip_file = "cleaned_purchases.zip"  # Replace with the uploaded ZIP file name
with zipfile.ZipFile(zip_file, 'r') as zip_ref:
    zip_ref.extractall("extracted_files")  # Extract to a folder named 'extracted_files'

# List extracted files
print("Extracted files:", os.listdir("extracted_files"))

from google.colab import files

# Upload the ZIP file
uploaded = files.upload()  # This will open the file upload dialog

# Assuming the uploaded file is named "cleaned_sales.zip"

import zipfile
import os

# Replace "cleaned_sales.zip" with the actual uploaded file name if necessary
zip_file = "cleaned_sales (3).zip"
with zipfile.ZipFile(zip_file, 'r') as zip_ref:
    zip_ref.extractall("extracted_files")  # Extract files into 'extracted_files' directory

# List the extracted files
print("Extracted files:", os.listdir("extracted_files"))

import json
import pandas as pd

# Path to the extracted JSON file
json_path = "extracted_files/cleaned_sales_data.json"  # Adjust if necessary

# Open and load the JSON file, handling potential JSON Lines format
data = []
with open(json_path, 'r') as f:
    for line in f:
        try:
            data.append(json.loads(line))  # Load each line as a separate JSON object
        except json.JSONDecodeError as e:
            print(f"Warning: Skipping invalid JSON line: {line.strip()} - Error: {e}")  # Print a warning for invalid lines

# Convert loaded data to a DataFrame
cleaned_sales_data = pd.DataFrame(data)


# Display the first few records
if isinstance(cleaned_sales_data, list):
    print(cleaned_sales_data[:5])  # Print the first 5 records
else:
    print("JSON content is not a list, here is the content:")
    print(cleaned_sales_data)

from google.colab import files

# Upload the CSV file
uploaded = files.upload()  # This will open the file upload dialog


import pandas as pd

# Specify the filename of the uploaded file
csv_file = list(uploaded.keys())[0]  # Automatically gets the uploaded file name

# Read the CSV into a DataFrame
cleaned_beginning_inventory = pd.read_csv(csv_file)

# Display the first few rows of the DataFrame
print(cleaned_beginning_inventory.head())

from google.colab import files

# Upload the CSV file
uploaded = files.upload()  # This will open the file upload dialog

import pandas as pd

# Specify the filename of the uploaded file
csv_file = list(uploaded.keys())[0]  # Automatically gets the uploaded file name

# Read the CSV into a DataFrame
cleaned_ending_inventory = pd.read_csv(csv_file)

# Display the first few rows of the DataFrame
print(cleaned_ending_inventory.head())

import os

# List all files and directories in the current directory
for file in os.listdir():
    print(file)

from google.colab import files
# Upload the ZIP file
uploaded = files.upload()  # This will open the file upload dialog


import zipfile
import os

# Extract the uploaded ZIP file
zip_file = "cleaned_purchases.zip"  # Replace with the uploaded ZIP file name
with zipfile.ZipFile(zip_file, 'r') as zip_ref:
    zip_ref.extractall("extracted_files")  # Extract to a folder named 'extracted_files'

# List extracted files
print("Extracted files:", os.listdir("extracted_files"))

import pandas as pd

# Path to the extracted CSV file
csv_path = "extracted_files/cleaned_purchases.csv"  # Adjust if necessary
cleaned_purchases = pd.read_csv(csv_path)

# Display the first few rows of the dataframe
print(cleaned_purchases.head())

import os

# List all files and directories in the current directory
for file in os.listdir():
    print(file)

import pandas as pd
import numpy as np
import zipfile
import os
import json

# Extract the uploaded ZIP file
zip_file = "cleaned_purchases.zip"  # Replace with the uploaded ZIP file name
with zipfile.ZipFile(zip_file, 'r') as zip_ref:
    zip_ref.extractall("extracted_files")
csv_path = "extracted_files/cleaned_purchases.csv"  # Adjust if necessary
purchases = pd.read_csv(csv_path)

# Load the cleaned data
end_inventory = pd.read_csv("cleaned_ending_inventory (2).csv")
beginning_inventory = pd.read_csv("cleaned_beginning_inventory.csv")

# Extract another ZIP file
zip_file = "cleaned_sales (3).zip"
with zipfile.ZipFile(zip_file, 'r') as zip_ref:
    zip_ref.extractall("extracted_files")  # Extract files into 'extracted_files' directory

# Path to the extracted JSON file
json_path = "extracted_files/cleaned_sales_data.json"  # Adjust if necessary

# Open and load the JSON file, handling potential JSON Lines format
data = []
with open(json_path, 'r') as f:
    for line in f:
        try:
            data.append(json.loads(line))  # Load each line as a separate JSON object
        except json.JSONDecodeError as e:
            print(f"Warning: Skipping invalid JSON line: {line.strip()} - Error: {e}")  # Print a warning for invalid lines

# Convert loaded data to a DataFrame
sales = pd.DataFrame(data)

# Surrogate key generation
# VendorDim
vendor_dim = purchases[['VendorNumber', 'VendorName']].drop_duplicates().reset_index(drop=True)  # Corrected here
vendor_dim.rename(columns={'VendorNumber': 'VendorId'}, inplace=True)

# DateDim
# Combine all date columns
date_columns = pd.concat([
    purchases['PODate'],
    purchases['ReceivingDate'],
    sales['SalesDate'],
    end_inventory['endDate'],
    beginning_inventory['startDate']
]).drop_duplicates().reset_index(drop=True)

# Ensure all date columns are in datetime format
date_dim = pd.DataFrame({'Date': pd.to_datetime(date_columns, errors='coerce')})
date_dim['DateId'] = range(1, len(date_dim) + 1)
date_dim['Day'] = date_dim['Date'].dt.day
date_dim['Month'] = date_dim['Date'].dt.month
date_dim['Quarter'] = date_dim['Date'].dt.quarter
date_dim['Year'] = date_dim['Date'].dt.year

# StoreDim
store_dim = end_inventory[['Store', 'City']].drop_duplicates().reset_index(drop=True)
store_dim['StoreId'] = range(1, len(store_dim) + 1)

# ProductDim
product_dim = end_inventory[['InventoryId', 'Brand']].drop_duplicates().reset_index(drop=True)
product_dim['ProductId'] = range(1, len(product_dim) + 1)

# Ensure 'startDate' in beginning_inventory is in datetime format
beginning_inventory['startDate'] = pd.to_datetime(beginning_inventory['startDate'], errors='coerce')

# Ensure 'Date' in date_dim is in datetime format
date_dim['Date'] = pd.to_datetime(date_dim['Date'], errors='coerce')

# Check for invalid dates in 'startDate' and handle them
invalid_start_dates = beginning_inventory[beginning_inventory['startDate'].isna()]
if not invalid_start_dates.empty:
    print("Warning: The following rows have invalid 'startDate' values and will be skipped:")
    print(invalid_start_dates)

# Remove rows with NaT in 'startDate' if needed (optional, based on your requirements)
beginning_inventory = beginning_inventory[~beginning_inventory['startDate'].isna()]

# Merge beginning_inventory with store_dim and product_dim to add 'StoreId' and 'ProductId'
beginning_inventory = beginning_inventory.merge(store_dim, on='Store', how='left')
beginning_inventory = beginning_inventory.merge(product_dim, on=['InventoryId', 'Brand'], how='left')

# Merge with date_dim on 'startDate'
beginning_inventory = beginning_inventory.merge(date_dim, left_on='startDate', right_on='Date', how='left')
beginning_inventory = beginning_inventory.rename(columns={'onHand': 'onHandStart'})

# Ensure 'endDate' in end_inventory is in datetime format
end_inventory['endDate'] = pd.to_datetime(end_inventory['endDate'], errors='coerce')

# Merge with date_dim on 'endDate'
end_inventory = end_inventory.merge(store_dim, on='Store', how='left')
end_inventory = end_inventory.merge(product_dim, on=['InventoryId', 'Brand'], how='left')
end_inventory = end_inventory.merge(date_dim, left_on='endDate', right_on='Date', how='left')
end_inventory = end_inventory.rename(columns={'onHand': 'onHandEnd'})

# Merge beginning and ending inventory
fact_inventory = pd.merge(
    beginning_inventory[['InventoryId', 'StoreId', 'ProductId', 'DateId', 'onHandStart']],
    end_inventory[['InventoryId', 'StoreId', 'ProductId', 'DateId', 'onHandEnd']],
    on=['InventoryId', 'StoreId', 'ProductId'],
    how='outer'
)

# Convert 'ReceivingDate' to datetime before merging
purchases['ReceivingDate'] = pd.to_datetime(purchases['ReceivingDate'], errors='coerce')

# Add StockReceived
purchases = purchases.merge(store_dim, on='Store', how='left')
purchases = purchases.merge(product_dim, on=['InventoryId', 'Brand'], how='left')
purchases = purchases.merge(date_dim, left_on='ReceivingDate', right_on='Date', how='left')
purchases['StockReceived'] = 1  # Each row represents one received stock

fact_inventory = fact_inventory.merge(
    purchases.groupby(['InventoryId', 'StoreId', 'ProductId'])['StockReceived'].sum().reset_index(),
    on=['InventoryId', 'StoreId', 'ProductId'],
    how='left'
)

# Add StockSold
sales = sales.merge(store_dim, on='Store', how='left')
sales = sales.merge(product_dim, on=['InventoryId', 'Brand'], how='left')

# Ensure 'SalesDate' in sales is in datetime format
sales['SalesDate'] = pd.to_datetime(sales['SalesDate'], errors='coerce')

# Ensure 'Date' in date_dim is in datetime format
date_dim['Date'] = pd.to_datetime(date_dim['Date'], errors='coerce')

# Merge with date_dim on 'SalesDate'
sales = sales.merge(date_dim, left_on='SalesDate', right_on='Date', how='left')

# Merge with vendor_dim
sales['StockSold'] = sales['SalesQuantity']

fact_inventory = fact_inventory.merge(
    sales.groupby(['InventoryId', 'StoreId', 'ProductId'])['StockSold'].sum().reset_index(),
    on=['InventoryId', 'StoreId', 'ProductId'],
    how='left'
)

# Fill missing values with 0
fact_inventory['onHandStart'] = fact_inventory['onHandStart'].fillna(0).astype(int)
fact_inventory['onHandEnd'] = fact_inventory['onHandEnd'].fillna(0).astype(int)
fact_inventory['StockReceived'] = fact_inventory['StockReceived'].fillna(0).astype(int)
fact_inventory['StockSold'] = fact_inventory['StockSold'].fillna(0).astype(int)

# Calculate StockTurnover
fact_inventory['StockTurnover'] = fact_inventory['StockSold'] / fact_inventory['onHandEnd'].replace(0, np.nan)

# FactSales
fact_sales = sales[['SalesQuantity', 'SalesDollars', 'SalesPrice', 'StoreId', 'ProductId', 'DateId', 'Volume', 'Description']]
fact_sales['SalesId'] = range(1, len(fact_sales) + 1)

# Save transformed data
vendor_dim.to_csv("VendorDim.csv", index=False)
date_dim.to_csv("DateDim.csv", index=False)
store_dim.to_csv("StoreDim.csv", index=False)
product_dim.to_csv("ProductDim.csv", index=False)
fact_inventory.to_csv("FactInventory.csv", index=False)
fact_sales.to_csv("FactSales.csv", index=False)

from google.colab import files

# List of CSV files to download
csv_files = [
     "FactSales.csv"
]

# Download each CSV file
for file in csv_files:
    files.download(file)

from google.colab import files

# Upload the CSV file
uploaded = files.upload()  # This will open the file upload dialog

import pandas as pd

# Specify the filename of the uploaded file
csv_file = list(uploaded.keys())[0]  # Automatically gets the uploaded file name

# Read the CSV into a DataFrame
cleaned_ending_inventory = pd.read_csv(csv_file)

# Display the first few rows of the DataFrame
print(cleaned_ending_inventory.head())

from google.colab import files

# Upload the CSV file
uploaded = files.upload()  # This will open the file upload dialog


import pandas as pd

# Specify the filename of the uploaded file
csv_file = list(uploaded.keys())[0]  # Automatically gets the uploaded file name

# Read the CSV into a DataFrame
cleaned_beginning_inventory = pd.read_csv(csv_file)

# Display the first few rows of the DataFrame
print(cleaned_beginning_inventory.head())

from google.colab import files
# Upload the ZIP file
uploaded = files.upload()  # This will open the file upload dialog


import zipfile
import os

# Extract the uploaded ZIP file
zip_file = "cleaned_purchases.zip"  # Replace with the uploaded ZIP file name
with zipfile.ZipFile(zip_file, 'r') as zip_ref:
    zip_ref.extractall("extracted_files")  # Extract to a folder named 'extracted_files'

# List extracted files
print("Extracted files:", os.listdir("extracted_files"))

import pandas as pd

# Path to the extracted CSV file
csv_path = "extracted_files/cleaned_purchases.csv"  # Adjust if necessary
cleaned_purchases = pd.read_csv(csv_path)

# Display the first few rows of the dataframe
print(cleaned_purchases.head())

# Assuming the uploaded file is named "cleaned_sales.zip"
import zipfile
import os

# Replace "cleaned_sales.zip" with the actual uploaded file name if necessary
zip_file = "cleaned_sales.zip"
with zipfile.ZipFile(zip_file, 'r') as zip_ref:
    zip_ref.extractall("extracted_files")  # Extract files into 'extracted_files' directory

# List the extracted files
print("Extracted files:", os.listdir("extracted_files"))
import json
import pandas as pd

# Path to the extracted JSON file
json_path = "extracted_files/cleaned_sales_data.json"  # Adjust if necessary

# Open and load the JSON file, handling potential JSON Lines format
data = []
with open(json_path, 'r') as f:
    for line in f:
        try:
            data.append(json.loads(line))  # Load each line as a separate JSON object
        except json.JSONDecodeError as e:
            print(f"Warning: Skipping invalid JSON line: {line.strip()} - Error: {e}")  # Print a warning for invalid lines

# Convert loaded data to a DataFrame
cleaned_sales_data = pd.DataFrame(data)


# Display the first few records
if isinstance(cleaned_sales_data, list):
    print(cleaned_sales_data[:5])  # Print the first 5 records
else:
    print("JSON content is not a list, here is the content:")
    print(cleaned_sales_data)

import pandas as pd
import numpy as np
import zipfile
import os
import json

# Extract the uploaded ZIP file
zip_file = "cleaned_purchases.zip"  # Replace with the uploaded ZIP file name
with zipfile.ZipFile(zip_file, 'r') as zip_ref:
    zip_ref.extractall("extracted_files")
csv_path = "extracted_files/cleaned_purchases.csv"  # Adjust if necessary
purchases = pd.read_csv(csv_path)

# Load the cleaned data
end_inventory = pd.read_csv("cleaned_ending_inventory (2).csv")
beginning_inventory = pd.read_csv("cleaned_beginning_inventory.csv")

# Extract another ZIP file
zip_file = "cleaned_sales.zip"
with zipfile.ZipFile(zip_file, 'r') as zip_ref:
    zip_ref.extractall("extracted_files")  # Extract files into 'extracted_files' directory

# Path to the extracted JSON file
json_path = "extracted_files/cleaned_sales_data.json"  # Adjust if necessary

# Open and load the JSON file, handling potential JSON Lines format
data = []
with open(json_path, 'r') as f:
    for line in f:
        try:
            data.append(json.loads(line))  # Load each line as a separate JSON object
        except json.JSONDecodeError as e:
            print(f"Warning: Skipping invalid JSON line: {line.strip()} - Error: {e}")  # Print a warning for invalid lines

# Convert loaded data to a DataFrame
sales = pd.DataFrame(data)

# Step 1: Prepare the dimension mappings for IDs
# Create ProductDim
product_dim = beginning_inventory[['InventoryId', 'Brand']].drop_duplicates().reset_index(drop=True)
product_dim['ProductId'] = range(1, len(product_dim) + 1)

# Create StoreDim
store_dim = beginning_inventory[['Store', 'City']].drop_duplicates().reset_index(drop=True)
store_dim['StoreId'] = range(1, len(store_dim) + 1)

# Create VendorDim using VendorNumber and VendorName
vendor_dim = purchases[['VendorNumber', 'VendorName']].drop_duplicates().reset_index(drop=True)
vendor_dim.rename(columns={'VendorNumber': 'VendorId'}, inplace=True)

# Create DateDim
def extract_date_parts(date_column):
    # Ensure date_column is a Series
    date_column = pd.Series(date_column)

    return pd.DataFrame({
        'DateId': range(1, len(date_column) + 1),
        'Day': date_column.dt.day,
        'Month': date_column.dt.month,
        'Quarter': date_column.dt.quarter,
        'Year': date_column.dt.year,
        # Include the original date column for merging
        'Date': date_column
    })

dates = pd.concat([sales['SalesDate'], purchases['ReceivingDate']])
dates = pd.to_datetime(dates.dropna().unique(), format='%d-%m-%Y', errors='coerce')
date_dim = extract_date_parts(dates)  # Pass dates as a Series


# Step 3: Create FactSales
# Map ProductId, StoreId, DateId, and VendorId
sales_fact = sales.copy()
sales_fact = sales_fact.merge(product_dim, on='InventoryId', how='left')
sales_fact = sales_fact.merge(store_dim, on='Store', how='left')
sales_fact['SalesDate'] = pd.to_datetime(sales_fact['SalesDate'], format='%d-%m-%Y', errors='coerce')

# Merge on 'SalesDate' and 'Date' (datetime columns)
sales_fact = sales_fact.merge(date_dim, left_on='SalesDate', right_on='Date', how='left')

sales_fact = sales_fact.merge(vendor_dim, on='VendorName', how='left')


# Fill null values with defaults
sales_fact['SalesQuantity'] = sales_fact['SalesQuantity'].fillna(0)
sales_fact['SalesDollars'] = sales_fact['SalesDollars'].fillna(0.0)
sales_fact['SalesPrice'] = sales_fact['SalesPrice'].fillna(0.0)
sales_fact['StoreId'] = sales_fact['StoreId'].fillna(-1).astype(int)
sales_fact['ProductId'] = sales_fact['ProductId'].fillna(-1).astype(int)
sales_fact['DateId'] = sales_fact['DateId'].fillna(-1).astype(int)

# Select relevant columns for FactSales
sales_fact_table = sales_fact[['SalesQuantity', 'SalesDollars', 'SalesPrice', 'StoreId', 'ProductId', 'DateId']]

# Step 4: Create FactInventory
# Merge beginning and ending inventory
inventory_fact = beginning_inventory.merge(end_inventory, on=['InventoryId', 'Store'], suffixes=('_start', '_end'), how='outer')

# Map ProductId and StoreId
inventory_fact = inventory_fact.merge(product_dim, on='InventoryId', how='left')  # Add ProductId
inventory_fact = inventory_fact.merge(store_dim, on='Store', how='left')  # Add StoreId

# Fill null values for start and end inventory
inventory_fact['onHandStart'] = inventory_fact['onHand_start'].fillna(0).astype(int)
inventory_fact['onHandEnd'] = inventory_fact['onHand_end'].fillna(0).astype(int)

# Calculate StockReceived and StockSold
stock_received = purchases.groupby('InventoryId').size().reindex(inventory_fact['InventoryId'], fill_value=0)
inventory_fact['StockReceived'] = stock_received.values

stock_sold = sales.groupby('InventoryId')['SalesQuantity'].sum().reindex(inventory_fact['InventoryId'], fill_value=0)
inventory_fact['StockSold'] = stock_sold.values

# Calculate StockTurnover (Avoid division by zero)
inventory_fact['StockTurnover'] = (
    inventory_fact['StockSold'] / inventory_fact['onHandStart'].replace(0, np.nan)
).fillna(0).round(2)

# Map DateId for endDate
inventory_fact['endDate'] = pd.to_datetime(inventory_fact['endDate'], errors='coerce')


# Merge with date_dim for endDate
inventory_fact = inventory_fact.merge(date_dim, left_on='endDate', right_on='Date', how='left', suffixes=('', '_end'))
inventory_fact.rename(columns={'DateId': 'EndDateId'}, inplace=True)

# Merge inventory_fact with purchases to get VendorName
inventory_fact = inventory_fact.merge(purchases[['InventoryId', 'VendorName']], on='InventoryId', how='left')

# Merge with vendor_dim to get VendorId
inventory_fact = inventory_fact.merge(vendor_dim, on='VendorName', how='left')

# Handle missing IDs and ensure clean data
inventory_fact['EndDateId'] = inventory_fact['EndDateId'].fillna(-1).astype(int)
inventory_fact['VendorId'] = inventory_fact['VendorId'].fillna(-1).astype(int)
inventory_fact['ProductId'] = inventory_fact['ProductId'].fillna(-1).astype(int)
inventory_fact['StoreId'] = inventory_fact['StoreId'].fillna(-1).astype(int)

# Select relevant columns for FactInventory
inventory_fact_table = inventory_fact[
    ['onHandStart', 'onHandEnd', 'StockReceived', 'StockSold', 'StockTurnover',
     'StoreId', 'ProductId', 'EndDateId', 'VendorId']
]

# Save to CSV
inventory_fact_table.to_csv('FactInventory.csv', index=False)
product_dim.to_csv('ProductDim.csv', index=False)
store_dim.to_csv('StoreDim.csv', index=False)
date_dim.to_csv('DateDim.csv', index=False)
vendor_dim.to_csv('VendorDim.csv', index=False)
sales_fact_table.to_csv('FactSales.csv', index=False)

# Download the CSV files
from google.colab import files
files.download('FactInventory.csv')

from google.colab import files

files.download("ProductDim.csv")

from google.colab import files

files.download("StoreDim.csv")

from google.colab import files

files.download("DateDim.csv")

from google.colab import files

files.download("VendorDim.csv")

from google.colab import files
files.download("FactSales.csv")

from google.colab import files
files.download("FactInventory.csv")